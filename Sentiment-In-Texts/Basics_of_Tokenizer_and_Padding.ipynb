{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"What companies Elon founded\",\n",
    "    \"Elon's company: Tesla, SpaceX, The Boring, OpenAI\",\n",
    "    \"We are the fehle of Elon Musk\",\n",
    "    \"Do you think Elon is the amazing?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "In this case, what it's going to do is, in your body of texts that it's tokenizing, \n",
    "it will take the 100 most common words or whatever value that you actually put in here.\n",
    "I have a lot less than a 100 unique words here, so it's not really going to have any effect. \n",
    "\n",
    "What fit on texts will then do is it will go through the entire body of text and it\n",
    "will create a dictionary with the key being the word and the value being the token for that word.\n",
    "\n",
    "# OOV Token\n",
    "\n",
    "I'm also going to use this parameter called an OOV token. The idea here is that I'm going to create a new token,\n",
    "a special token that I'm going to use for words that aren't recognized, that aren't in the word index itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\") \n",
    "tokenizer.fit_on_texts(sentences) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Indexing and Sequences\n",
    "\n",
    "Number one is that punctuation like spaces and the comma, have actually been removed.\n",
    "So it cleans up my text for me in that way too just to actually pull out the words.\n",
    "\n",
    "Tokenizer in Tensorflow doesn't care about lowercase or uppercase letters. \n",
    "\n",
    "One really handy thing about this that you'll use later is the fact that the text to sequences\n",
    "called can take any set of sentences, so it can encode them based on the word set that it\n",
    "learned from the one that was passed into fit on texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'elon': 2, 'the': 3, 'what': 4, 'companies': 5, 'founded': 6, \"elon's\": 7, 'company': 8, 'tesla': 9, 'spacex': 10, 'boring': 11, 'openai': 12, 'we': 13, 'are': 14, 'fehle': 15, 'of': 16, 'musk': 17, 'do': 18, 'you': 19, 'think': 20, 'is': 21, 'amazing': 22}\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 5, 2, 6], [7, 8, 9, 10, 3, 11, 12], [13, 14, 3, 15, 16, 2, 17], [18, 19, 20, 2, 21, 3, 22]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data\n",
    "\n",
    "So now, if I want to take a look at words that the tokenizer wasn't fit to. So for example, my test data is \"what can I do with this notebook\" and \"my lover really likes work of the Elon\", if I now tokenized them and create sequences out of that, we'll see  [4, 1, 1, 18, 1, 1, 1]  for the first sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 1, 1, 18, 1, 1, 1], [1, 1, 1, 1, 1, 16, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    'what can I do with this notebook',\n",
    "    'my lover really likes work of the Elon'\n",
    "]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq) # 1 is the \"404 text\" and assigned to the unknown words from sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "\n",
    "You can now see that the list of sentences has been padded out into a matrix and that\n",
    "each row in the matrix has the same length. It achieved this by putting the appropriate number of zeros before the sentence. So in the case of the sentence 4, 5, 2, 6, it didn't actually do any. In the case of the longer sentence\n",
    "here it didn't need to do any. Often you'll see examples where the padding is after the sentence and not before as you just saw. \n",
    "\n",
    "\n",
    "But you can override that with the maxlen parameter. So for example if you only want your sentences to have a maximum of five words. You can say maxlen equals five like this. This of course will lead to the question.\n",
    "If I have sentences longer than the maxlength, then I'll lose information but from where. Like with the padding the default is pre, which means that you will lose from the beginning of the sentence. If you want to override this so that you lose from the end instead, you can do so with the truncating parameter like this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  5  2  6  0  0  0]\n",
      " [ 7  8  9 10  3 11 12]\n",
      " [13 14  3 15 16  2 17]\n",
      " [18 19 20  2 21  3 22]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding=\"post\", maxlen=7)\n",
    "print(padded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
